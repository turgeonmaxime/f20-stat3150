---
title: "Optimisation"
draft: true
source: true
output: binb::metropolis
fontsize: 12pt
author: Max Turgeon
institute: STAT 3150--Statistical Computing
header-includes:
  - \usefonttheme{professionalfonts}
  - \usepackage{graphicx}
  - \usepackage{tikzpagenodes}
  - \usetikzlibrary{calc}
  - \usepackage{caption}
  - \DeclareMathOperator*{\argmax}{arg\,max}
  - \DeclareMathOperator*{\argmin}{arg\,min}
---

```{r,setup, include=FALSE}
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

knitr::opts_chunk$set(cache = FALSE, message = FALSE,
                      linewidth = 50)
```

## Lecture Objectives

  - Give examples of optimisation problems in statistics.
  - Apply the Newton-Raphson algorithm to compute Maximum Likelihood estimates.

## Motivation

  - **Optimisation** is a very old and broad field of applied mathematics.
    + It is older than calculus: see Heron's problem.
  - It plays an important role in statistics, but also computational biology, economics, mathematical finance, etc.
  - Our discussion will necessarily be brief, but I will try to give a sample of the types of statistical problems that can be solved using optimisation.
    + For more details, I recommend *Optimization* (2013) by Kenneth Lange.
    
## Problem statement

  - In very general terms, the basic optimisation problem is as follows.
  - Let $A$ be a set (e.g. a subset of $\mathbb{R}^n$, a set of integers, etc.), and let $f$ be a real-valued function on $A$.
  - We are looking for a value $x_0\in A$ such that:
    + **Minimisation**: $f(x_0) \leq f(x)$ for all $x\in A$.
    + **Maximisation**: $f(x_0) \geq f(x)$ for all $x\in A$.
  - The set $A$ can be defined via a combination of equality and inequality constraints.
    + For a multinomial model, we want $K$ probabilities $p_k$ such that $0\leq p_k \leq 1$ and $\sum_{k=1}^K p_k = 1$.
    
## Lasso regression

  - In linear regression, we find an estimate $\hat{\beta}$ by minimising least squares:
  $$\hat{\beta} = \argmin_{\beta \in \mathbb{R}^p} \sum_{i=1}^n \left(Y_i - \mathbf{X}_i^T \beta\right)^2.$$
  - In lasso regression, we add a constraint on the L1 norm of $\beta$:
  $$\hat{\beta}_{Lasso} = \argmin_{\beta \in \mathbb{R}^p} \sum_{i=1}^n \left(Y_i - \mathbf{X}_i^T \beta\right)^2, \quad \sum_{j=1}^p \lvert \beta_j \rvert \leq \lambda.$$
  - **Why?** By adding the L1 constraint, some of the components of the solution $\hat{\beta}_{Lasso}$ can be exactly zero, leading to *variable selection*.
  
## Dimension reduction

  - Let $\mathbf{Y}$ be a $p$-dimensional random vector. 
  - In dimension reduction, we are looking for a linear combination $w^T\mathbf{Y}$ that optimises a given criterion.
  - For example:
    + In **Principal Component Analysis**, we are looking to maximise $\mathrm{Var}\left(w^T\mathbf{Y}\right)$.
    + In **Independent Component Analysis**, we are looking to minimise mutual information. 

## Mixture models

  - Recall from earlier this semester *mixture models*:
  $$F(x) = \sum_{k=1}^K \pi_k F_k(x),$$
  where $F_k$ is a distribution function for all $k$ and $\sum_{k=1}^K \pi_k = 1$.
  - The likelihood can be difficult to maximise directly.
  - Instead, we typically use the **Expectation-Maximization algorithm**.

## Machine learning

  - A common goal in machine learning is *prediction*: given a vector of features $\mathbf{X}$, find a function $f$ such that 
  $$L(Y, f(\mathbf{X}))$$
  is minimised. 
  - Here $L(Y, \cdot)$ is a *loss function*. For example:
    + $L(Y, f(\mathbf{X})) = (Y - f(\mathbf{X}))^2$
    + $L(Y, f(\mathbf{X})) = \lvert Y - f(\mathbf{X})\rvert$

## Newton-Raphson method {.allowframebreaks}

  - The Newton-Raphson method is the only optimisation technique we will discuss.
    + It is a very important technique and serves as motivation for more advanced methods.
  - Suppose we have a twice differentiable function $f: A \to \mathbb{R}$, where $A\subseteq \mathbb{R}^n$. We want to find the maximum of $f$ on $A$.
  - We can approximate $f$ using a Taylor expansion:
  $$f(\mathbf{x} + \mathbf{h}) \approx f(\mathbf{x}) + \nabla f(\mathbf{x})^T \mathbf{h} + \frac{1}{2}\mathbf{h}^TD^2f(\mathbf{x})\mathbf{h},$$
  where $\nabla f(\mathbf{x})$ is the vector of first-order derivatives and $D^2f(\mathbf{x})$ is the matrix of second-order derivatives.
  - They are called the **gradient** and the **Hessian**, respectively.
  - In other words, we are approximating $f$ in a neighbourhood of $\mathbf{x}$ using a quadratic function of $\mathbf{h}$.
  - We know how to find the maximum of a quadratic function: take the derivative, set it equal to zero, and solve for $\mathbf{h}$.
    + **Why?** The derivative of a quadratic is a linear function. Easy to solve!
  - The derivative of the Taylor approximation with respect to $\mathbf{h}$ is
  $$\frac{d}{d\mathbf{h}} \left[f(\mathbf{x}) + \nabla f(\mathbf{x})^T \mathbf{h} + \frac{1}{2}\mathbf{h}^TD^2f(\mathbf{x})\mathbf{h}\right] = \nabla f(\mathbf{x}) + D^2f(\mathbf{x})\mathbf{h}.$$
  - Setting the derivative equal to zero and solving for $\mathbf{h}$ gives
  $$\mathbf{h} = -[D^2f(\mathbf{x})]^{-1}\nabla f(\mathbf{x}).$$
  - Therefore, the *approximate* maximum of our function $f$ is a neighbourhood of $\mathbf{x}$ occurs at
  $$\mathbf{x} + \mathbf{h} = \mathbf{x} - [D^2f(\mathbf{x})]^{-1}\nabla f(\mathbf{x}).$$
  - In one-dimension, this simplifies to
  $$x + h = x - \frac{f^{\prime}(x)}{f^{\prime\prime}(x)}.$$
  
### Algorithm

Let $\mathbf{x}_0$ be an initial value. Then construct a sequence via
$$\mathbf{x}_{n+1} = \mathbf{x}_n - [D^2f(\mathbf{x}_n)]^{-1}\nabla f(\mathbf{x}_n).$$

If the initial value $\mathbf{x}_0$ is "sufficiently close" to a maximum (or minimum) of $f$, then the sequence $\mathbf{x}_n$ will converge to that maximum (or minimum).

## Example 1 {.allowframebreaks}

  - Consider the function $f(x) = -x^2 + \sin(x)$.
  - The derivatives are
  $$f^\prime(x) = -2x + \cos(x),\quad f^{\prime\prime}(x) = -2 - \sin(x).$$
  
```{r}
# Create functions
gradient <- function(x) -2*x + cos(x)
hessian <- function(x) -2 - sin(x)
# Set-up parameters
x_current <- 1
x_next <- 0
tol <- 10^-10
iter <- 0
```


```{r}
while (abs(x_current - x_next) > tol &
       iter < 100) {
  iter <- iter + 1
  step <- gradient(x_current)/hessian(x_current)
  update <- x_current - step
  x_current <- x_next
  x_next <- update
}
x_next
```

```{r}
plot(function(x) -x^2 + sin(x), from = -1, to = 1)
abline(v = x_next, lty = 2)
```

## When does it go wrong?

  - Newton-Raphson usually goes wrong in one of two ways:
    + $\nabla f(\mathbf{x}_0) = 0$ and you're stuck at $\mathbf{x}_0$.
    + The sequence $\mathbf{x}_n$ does not converge.
  - That's why we kept track of the number of iterations, so that we aren't stuck in an infinite loop.
  - When you reach the maximum number of iterations, you can:
    + Try a different initial value.
    + Look at a graph and see if there is indeed a maximum/minimum.
    + Consider using a different approach (e.g. gradient descent).
    
## Example (not convergent)

  - Consider $f(x) = 0.25x^4 - x^2 + 2x$.
  - The derivatives are
  $$f^\prime(x) = x^3 - 2x + 2, \quad f^{\prime\prime}(x) = 3x^2 - 2.$$
  - If we start at $x_0 = 0$, then at the next iteration we get
  $$x_1 = x_0 - \frac{f^\prime(x_0)}{f^{\prime\prime}(x_0)} = 0 - \frac{2}{-2} = 1.$$
  - Next we get
  $$x_2 = x_1 - \frac{f^\prime(x_1)}{f^{\prime\prime}(x_1)} = 1 - \frac{1}{1} = 0.$$
  - Therefore, we are stuck in an infinite loop.
    + **Solution**: Use a different starting value.

## Exercise

Find the minimum for the function 
$$f(x) = 5x + 5x^2 + 5\log(1 + e^{âˆ’x}).$$

## Solution {.allowframebreaks}

We need to compute the first and second derivatives:

\begin{align*}
f^\prime(x) &= 5 + 10x - \frac{5e^{-x}}{1 + e^{-x}} = 5 + 10x - \frac{5}{1 + e^{x}},\\
f^{\prime\prime} &= 10 + \frac{5e^x}{(1 + e^{x})^2}.
\end{align*}

```{r}
gradient <- function(x) 5 + 10*x - 5/(1+exp(x))
hessian <- function(x) 10 + 5*exp(x)/(1+exp(x))^2

x_current <- 1
x_next <- 0
tol <- 10^-10
iter <- 0
```


```{r}
while (abs(x_current - x_next) > tol &
       iter < 100) {
  iter <- iter + 1
  step <- gradient(x_current)/hessian(x_current)
  update <- x_current - step
  x_current <- x_next
  x_next <- update
}
x_next
```

## Poisson regression {.allowframebreaks}

  - **Poisson regression** is a generalization of linear regression where the outcome variable follows a Poisson distribution. 
  - Let $(Y_i, X_i)$, $i=1,\ldots,n$, be pairs of outcome and covariate variables. We assume the following relationship:
  $$ Y_i \sim Pois(\mu(X_i)), \qquad \log(\mu(X_i)) = \beta_0 + \beta_1 X_i.$$
  - We will use the Newton-Raphson method to find the Maximum Likelihood estimate $\hat{\theta} = (\hat{\beta}_0, \hat{\beta}_1)$.
  - First, let's write down the likelihood:
  $$L(\theta) = \prod_{i=1}^n\frac{\mu(X_i)^{Y_i}\exp\left(\mu(X_i)\right)}{Y_i!}.$$
  - Take the logarithm:
  
\begin{align*}
\ell(\theta) &= \sum_{i=1}^n Y_i\log(\mu(X_i)) - \mu(X_i) - \log(Y_i!)\\
  &= \sum_{i=1}^n Y_i\left(\beta_0 + \beta_1 X_i\right) - \exp(\beta_0 + \beta_1 X_i) - \log(Y_i!)\\
  &\propto \sum_{i=1}^n Y_i\left(\beta_0 + \beta_1 X_i\right) - \exp(\beta_0 + \beta_1 X_i).
\end{align*}

\vspace{1cm}

  - The first-order derivatives are
  
\begin{align*}
\frac{\partial\ell(\theta)}{\partial\beta_0} &= \sum_{i=1}^n Y_i - \exp(\beta_0 + \beta_1 X_i),\\
\frac{\partial\ell(\theta)}{\partial\beta_1} &= \sum_{i=1}^n Y_iX_i - X_i\exp(\beta_0 + \beta_1 X_i).
\end{align*}

\vspace{2.5cm}

  - The second-order derivatives are
  
\begin{align*}
\frac{\partial^2\ell(\theta)}{\partial\beta_0^2} &= -\sum_{i=1}^n\exp(\beta_0 + \beta_1 X_i),\\
\frac{\partial^2\ell(\theta)}{\partial\beta_1^2} &= -\sum_{i=1}^nX_i^2\exp(\beta_0 + \beta_1 X_i),\\
\frac{\partial^2\ell(\theta)}{\partial\beta_0\partial\beta_1} &= -\sum_{i=1}^nX_i\exp(\beta_0 + \beta_1 X_i).
\end{align*}

\vspace{2cm}

  - We will fit a Poisson regression model to the "famous" horseshoe crab dataset. 
    + The main covariate is the weight of female crabs.
    + The outcome is the number of satellite males for that female crab.
    + The dataset can be found in the `asbio` package.

```{r}
library(asbio)
data(crabs)
y_vec <- crabs$satell
x_vec <- crabs$weight

gradient <- function(theta) {
  mu_x <- exp(theta[1] + theta[2]*x_vec)
  ell_b0 <- sum(y_vec - mu_x)
  ell_b1 <- sum(y_vec*x_vec - x_vec*mu_x)
  return(c(ell_b0, ell_b1))
}
```


```{r}
hessian <- function(theta) {
  mu_x <- exp(theta[1] + theta[2]*x_vec)
  ell_b0b0 <- -sum(mu_x)
  ell_b1b1 <- -sum(x_vec^2*mu_x)
  ell_b0b1 <- -sum(x_vec*mu_x)
  
  hess <- matrix(c(ell_b0b0, ell_b0b1,
                   ell_b0b1, ell_b1b1),
                 ncol = 2)
  return(hess)
}
```


```{r}
# Set-up variables
x_current <- c(1, 1)
x_next <- c(0, 0)
tol <- 10^-10
iter <- 0
```


```{r}
while(sum((x_current - x_next)^2) > tol & iter < 100) {
  iter <- iter + 1
  step <- solve(hessian(x_current), 
                gradient(x_current))
  update <- x_current - step
  x_current <- x_next
  x_next <- update
}
x_next
```

  - We can compare this to the built-in function `glm` in `R`:
  
```{r}
fit <- glm(satell ~ weight, data = crabs,
           family = poisson)
coef(fit)
```

## Some vocabulary

  - In the context of Maximum Likelihood estimation, the gradient and Hessian are given different names.
  - The gradient $\nabla \ell(\theta\mid\mathbf{x})$ is called the **score function**.
    + **Exercise**: The expected value of the score function is 0.
  - The Hessian $D^2\ell(\theta\mid\mathbf{x})$ is related to the Fisher information matrix $\mathcal{I}(\theta)$:
  $$\mathcal{I}(\theta) = -E\left(D^2\ell(\theta\mid\mathbf{x})\right).$$

<!-- ## Example 2 {.allowframebreaks} -->

<!--   - Blood types come broadly in 4 types: A, B, AB, and O. -->
<!--   - Genetically speaking, your blood type is determined by the two alleles you received from your parents. -->
<!--     + Possible alleles: A, B or O -->
<!--     + A,B are each dominant over O (e.g. if you received B and O alleles, your blood type is B). -->
<!--     + A,B are codominant, i.e. if you received A and B alleles, your blood type is AB. -->
<!--   - Let $p_A,p_B,p_O$ be the frequencies of each allele in the population. -->
<!--   - Let $n_A,n_B,n_{AB},n_O$ be the number of observed blood types in a certain sample. The multinomial likelihood for $\theta = (p_A,p_B,p_O)$ is -->

<!-- \begin{align*} -->
<!-- L(\theta) &= {n\choose n_A,n_B,n_{AB},n_O} (p_A^2 + 2p_Ap_O)^{n_A}\cdot(p_B^2 + 2p_Bp_O)^{n_B}\\ -->
<!--   &\qquad \cdot(2p_Ap_B)^{n_{AB}}\cdot(p_O^2)^{n_O}, -->
<!-- \end{align*} -->
<!--   where $n = n_A + n_B + n_{AB} + n_O$. -->

<!-- \vspace{2cm} -->

<!--   - We will make two simplifications before taking any derivative: -->
<!--     + We don't need the constant ${n\choose n_A,n_B,n_{AB},n_O}$. -->
<!--     + We will use the identity $p_O = 1 - p_A - p_B$, and substitute $\theta = (p_A, p_B)$. -->

<!-- \begin{align*} -->
<!-- \ell(\theta) &= n_A\log(p_A^2 + 2p_Ap_O) + n_B\log(p_B^2 + 2p_Bp_O)\\ -->
<!--   &\quad + n_{AB}\log(2p_Ap_B) + 2n_O\log(p_O).\\ -->
<!-- \end{align*} -->

<!-- \vspace{1cm} -->

<!--   - The first-order derivatives are -->

<!-- \begin{align*} -->
<!-- \frac{\partial}{\partial p_A}\ell(\theta) &= \frac{2n_Ap_O}{p_A^2 + 2p_Ap_O} - \frac{2n_B}{p_B + 2p_O} + \frac{n_{AB}}{p_A} - \frac{2n_O}{p_O};\\ -->
<!-- \frac{\partial}{\partial p_B}\ell(\theta) &= -\frac{2n_A}{p_A + 2p_O} + \frac{2n_Bp_O}{p_B^2 + 2p_Bp_O} + \frac{n_{AB}}{p_B} - \frac{2n_O}{p_O}.\\ -->
<!-- \end{align*} -->

<!-- \vspace{2cm} -->

<!--   - The second-order derivatives are -->

<!-- \begin{align*} -->
<!-- \frac{\partial^2}{\partial p_A^2}\ell(\theta) &= -\frac{2n_A(p_A^2 + p_Ap_O + 2p_O^2)}{(p_A^2 + 2p_Ap_O)^2} - \frac{4n_B}{(p_B + 2p_O)^2}\\ -->
<!--   &\quad - \frac{n_{AB}}{p_A^2} - \frac{2n_O}{p_O^2};\\ -->
<!-- \frac{\partial^2}{\partial p_B^2}\ell(\theta) &= - \frac{4n_A}{(p_A + 2p_O)^2} -\frac{2n_B(p_B^2 + p_Bp_O + 2p_O^2)}{(p_B^2 + 2p_Bp_O)^2}\\ -->
<!--   &\quad - \frac{n_{AB}}{p_B^2} - \frac{2n_O}{p_O^2};\\ -->
<!-- \frac{\partial^2}{\partial p_A\partial p_B}\ell(\theta) &= -\frac{2n_A}{(p_A + 2p_O)^2} -\frac{2n_B}{(p_B + 2p_O)^2} -\frac{2n_O}{p_O^2}. -->
<!-- \end{align*} -->

<!--   - Now that we have the gradient and Hessian, we can use Newton-Raphson to find the Maximum Likelihood estimate of $\theta = (p_A, p_B)$. -->
<!--   - Our data is as follows: $n_A = 186, n_B = 38, n_{AB} = 13, n_O = 284$. -->

<!-- ```{r} -->
<!-- gradient <- function(theta) { -->
<!--   pA <- theta[1] -->
<!--   pB <- theta[2] -->
<!--   pO <- 1 - pA - pB -->
<!--   ell_A <- 2*nobs[1]*pO/(pA^2 + 2*pA*pO) - -->
<!--     2*nobs[2]/(pB + 2*pO) + nobs[3]/pA - 2*nobs[4]/pO -->
<!--   ell_B <- - 2*nobs[1]/(pA + 2*pO) + -->
<!--     2*nobs[2]*pO/(pB^2 + 2*pB*pO) + nobs[3]/pB -  -->
<!--     2*nobs[4]/pO -->
<!--   return(c(ell_A, ell_B)) -->
<!-- } -->
<!-- ``` -->


<!-- ```{r} -->
<!-- hessian <- function(theta) { -->
<!--   pA <- theta[1] -->
<!--   pB <- theta[2] -->
<!--   pO <- 1 - pA - pB -->
<!--   ell_AA <- -2*nobs[1]*(pA^2 + pA*pO + 2*pO^2)/(pA^2 + 2*pA*pO)^2 - -->
<!--     4*nobs[2]/(pB+2*pO)^2 - nobs[3]/pA^2 - 2*nobs[4]/pO^2 -->
<!--   ell_BB <- -2*nobs[2]*(pB^2 + pB*pO + 2*pO^2)/(pB^2+2*pB*pO)^2 - -->
<!--     4*nobs[1]/(pA+2*pO)^2 - nobs[3]/pB^2 - 2*nobs[4]/pO^2 -->
<!--   ell_AB <- -2*nobs[1]/(pA^2+2*pO^2)^2 - -->
<!--     2*nobs[2]/(pB^2+2*pO^2)^2 - 2*nobs[4]/pO^2 -->
<!--   return(matrix(c(ell_AA, ell_AB, ell_AB, ell_BB), -->
<!--                 nrow = 2)) -->
<!-- } -->
<!-- ``` -->


<!-- ```{r} -->
<!-- nobs <- c(186, 38, 13, 284) -->
<!-- x_current <- c(0.3, 0.2) -->
<!-- x_next <- c(0, 0) -->
<!-- tol <- 10^-10 -->
<!-- iter <- 0 -->
<!-- ``` -->

<!-- ```{r eval = FALSE} -->
<!-- while (sum((x_current - x_next)^2) > tol & -->
<!--        iter < 100) { -->
<!--   iter <- iter + 1 -->
<!--   step <- solve(hessian(x_current), gradient(x_current)) -->
<!--   update <- x_current - step -->
<!--   x_current <- x_next -->
<!--   x_next <- update -->
<!-- } -->
<!-- x_next -->
<!-- ``` -->

## Final remarks

  - Newton-Raphson is an important optimisation method.
    + In particular, it is commonly used with *generalized linear models*.
  - There exists optimisation methods that are **gradient-free**.
    + See Nelder-Mead and the function `optim`.
  - Where to go from here:
    + Gradient descent
    + Regularized regression (e.g. lasso)
    + MM algorithms
  